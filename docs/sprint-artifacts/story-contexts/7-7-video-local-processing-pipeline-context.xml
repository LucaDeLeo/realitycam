<?xml version="1.0" encoding="UTF-8"?>
<!--
  Story Context: 7-7-video-local-processing-pipeline
  Generated: 2025-11-27
  Purpose: Single source of truth for implementing video local processing pipeline
-->
<story-context>
  <story-reference>
    <story-id>7-7-video-local-processing-pipeline</story-id>
    <story-file>docs/sprint-artifacts/stories/7-7-video-local-processing-pipeline.md</story-file>
    <epic>Epic 7 - Video Capture with LiDAR Depth</epic>
    <priority>P0</priority>
    <estimated-effort>M</estimated-effort>
  </story-reference>

  <epic-context>
    <tech-spec>docs/sprint-artifacts/epic-tech-specs/tech-spec-epic-7.md</tech-spec>
    <architecture>docs/architecture.md</architecture>
    <dependencies>
      <dependency story="7-4">Frame Hash Chain - HashChainData, HashCheckpoint structs</dependency>
      <dependency story="7-5">Video Attestation Checkpoints - VideoAttestation, VideoAttestationService</dependency>
      <dependency story="7-6">Video Metadata Collection - VideoMetadata, VideoMetadataCollector</dependency>
      <dependency story="7-1">ARKit Video Recording Session - VideoRecordingResult, VideoRecordingSession</dependency>
      <dependency story="7-2">Depth Keyframe Extraction - DepthKeyframeBuffer, DepthKeyframeData</dependency>
      <dependency story="6-9">CoreData Capture Persistence - CaptureStore patterns</dependency>
    </dependencies>
  </epic-context>

  <story-requirements>
    <user-story>
      As a developer, I want video captures processed and packaged for upload,
      so that all components are correctly assembled for backend verification.
    </user-story>

    <acceptance-criteria>
      <criterion id="AC-7.7.1">
        <title>Video Processing Components</title>
        <given>Video recording has completed with VideoRecordingResult</given>
        <when>Local processing runs</when>
        <then>
          - Video file (H.264/HEVC, ~10-30MB for 15s 1080p) - already at result.videoURL
          - Compressed depth data blob (gzip, ~10MB)
          - Serialized hash chain data (JSON with all frame hashes)
          - Checkpoint hashes for partial verification
          - Metadata JSON with attestation
          - Thumbnail image (first frame, 640x360)
        </then>
      </criterion>
      <criterion id="AC-7.7.2">
        <title>Processing Performance</title>
        <given>A 15-second video recording completes</given>
        <when>Local processing runs</when>
        <then>
          - Total processing time less than 5 seconds
          - UI remains responsive during processing
          - Memory usage does not spike above 50MB additional
          - Progress callback provides status updates
        </then>
      </criterion>
      <criterion id="AC-7.7.3">
        <title>Depth Data Compression</title>
        <given>Raw depth keyframes (~30MB for 150 frames at 10fps)</given>
        <when>Compression runs</when>
        <then>
          - Gzip compression applied to depth blob
          - Compressed size ~10MB (66% reduction)
          - Compression uses streaming to avoid memory spikes
          - Original depth data preserved in memory for preview
        </then>
      </criterion>
      <criterion id="AC-7.7.4">
        <title>Thumbnail Generation</title>
        <given>A completed video recording</given>
        <when>Thumbnail is generated</when>
        <then>
          - First frame extracted from video file
          - Thumbnail resized to 640x360 (maintains aspect ratio)
          - JPEG compression at 80% quality
          - Thumbnail saved alongside video for preview/history
        </then>
      </criterion>
      <criterion id="AC-7.7.5">
        <title>Package Assembly</title>
        <given>All components are processed</given>
        <when>Package assembly completes</when>
        <then>
          ProcessedVideoCapture contains:
          - Video file URL (original from recording)
          - Compressed depth data blob
          - Serialized hash chain (JSON)
          - Metadata with attestation
          - Thumbnail image data
          - Capture ID (UUID)
          - Status: ready_for_upload
        </then>
      </criterion>
      <criterion id="AC-7.7.6">
        <title>CoreData Persistence</title>
        <given>A processed video capture</given>
        <when>Saved to local storage</when>
        <then>
          - Video capture record created in CoreData
          - Status set to "pending_upload"
          - All file URLs stored correctly
          - Supports offline queue integration (Story 7-8)
        </then>
      </criterion>
    </acceptance-criteria>

    <fr-coverage>
      <fr id="FR51" primary="true">App processes video for backend verification (packaging video, depth, chain, metadata)</fr>
      <fr id="FR47">Video file ready for upload</fr>
      <fr id="FR49">Hash chain serialized for backend verification</fr>
      <fr id="FR50">Attestation packaged with metadata</fr>
      <fr id="FR52">Backend can verify uploaded video package</fr>
    </fr-coverage>
  </story-requirements>

  <technical-interfaces>
    <interface name="VideoRecordingResult">
      <description>Input from Story 7-1 - complete video recording result</description>
      <source-file>ios/Rial/Core/Capture/VideoRecordingSession.swift</source-file>
      <code-snippet><![CDATA[
public struct VideoRecordingResult: Sendable {
    /// URL of the recorded video file
    public let videoURL: URL

    /// Number of frames captured
    public let frameCount: Int

    /// Recording duration in seconds
    public let duration: TimeInterval

    /// Video resolution
    public let resolution: (width: Int, height: Int)

    /// Codec used for encoding
    public let codec: String

    /// Whether recording was interrupted
    public let wasInterrupted: Bool

    /// Recording start time
    public let startedAt: Date

    /// Recording end time
    public let endedAt: Date

    /// Depth keyframe data captured at 10fps (optional, may be nil if no depth data)
    public let depthKeyframeData: DepthKeyframeData?

    /// Hash chain data for frame-by-frame cryptographic integrity (optional)
    public let hashChainData: HashChainData?

    /// Video attestation (DCAppAttest signature, optional if attestation failed)
    let attestation: VideoAttestation?

    /// Video metadata for evidence package (device info, location, timestamps)
    public let metadata: VideoMetadata

    /// Number of depth keyframes captured (convenience property)
    public var depthKeyframeCount: Int {
        depthKeyframeData?.keyframeCount ?? 0
    }

    /// Final hash for attestation signing (convenience property)
    public var finalHash: Data? {
        hashChainData?.finalHash
    }

    /// Whether this is a partial recording (convenience property)
    public var isPartial: Bool {
        attestation?.isPartial ?? false
    }
}
]]></code-snippet>
    </interface>

    <interface name="HashChainData">
      <description>Hash chain data from Story 7-4 - needs JSON serialization</description>
      <source-file>ios/Rial/Core/Crypto/HashChainService.swift</source-file>
      <code-snippet><![CDATA[
public struct HashChainData: Codable, Sendable {
    /// All frame hashes at 30fps (up to 450 for 15-second video)
    public let frameHashes: [Data]

    /// Checkpoint hashes every 5 seconds (up to 3)
    public let checkpoints: [HashCheckpoint]

    /// Last frame hash for attestation signing (32 bytes)
    public let finalHash: Data

    /// Total number of frames in chain
    public var frameCount: Int { frameHashes.count }

    /// Number of checkpoints stored
    public var checkpointCount: Int { checkpoints.count }
}

public struct HashCheckpoint: Codable, Sendable, Equatable {
    /// Checkpoint index (0=5s, 1=10s, 2=15s)
    public let index: Int

    /// Frame number at checkpoint (150, 300, 450)
    public let frameNumber: Int

    /// Chain hash at this checkpoint (32 bytes SHA256)
    public let hash: Data

    /// Video timestamp in seconds (relative to recording start)
    public let timestamp: TimeInterval
}
]]></code-snippet>
    </interface>

    <interface name="VideoAttestation">
      <description>Attestation from Story 7-5 - needs JSON serialization</description>
      <source-file>ios/Rial/Core/Attestation/VideoAttestationService.swift</source-file>
      <code-snippet><![CDATA[
struct VideoAttestation: Codable, Equatable, Sendable {
    /// Hash that was attested (final hash for complete, checkpoint hash for partial)
    let finalHash: Data

    /// DCAppAttest signature (CBOR-encoded assertion)
    let assertion: Data

    /// Attested duration in milliseconds (may be partial if interrupted)
    let durationMs: Int64

    /// Attested frame count (may be partial if interrupted)
    let frameCount: Int

    /// True if recording was interrupted and only partial video is attested
    let isPartial: Bool

    /// Which checkpoint was attested (0=5s, 1=10s, 2=15s), nil for complete recordings
    let checkpointIndex: Int?

    /// Base64-encoded assertion for JSON serialization.
    var assertionBase64: String {
        assertion.base64EncodedString()
    }

    /// Base64-encoded final hash for logging and metadata.
    var finalHashBase64: String {
        finalHash.base64EncodedString()
    }
}
]]></code-snippet>
    </interface>

    <interface name="VideoMetadata">
      <description>Metadata from Story 7-6 - already JSON serializable with snake_case</description>
      <source-file>ios/Rial/Models/VideoMetadata.swift</source-file>
      <code-snippet><![CDATA[
public struct VideoMetadata: Codable, Equatable, Sendable {
    /// Media type identifier (always "video")
    public let type: String

    /// Recording start timestamp (UTC)
    public let startedAt: Date

    /// Recording end timestamp (UTC)
    public let endedAt: Date

    /// Total recording duration in milliseconds
    public let durationMs: Int64

    /// Total frame count (30fps * duration)
    public let frameCount: Int

    /// Depth keyframe count (10fps * duration)
    public let depthKeyframeCount: Int

    /// Video resolution
    public let resolution: Resolution

    /// Video codec ("h264" or "hevc")
    public let codec: String

    /// Device model (e.g., "iPhone 15 Pro")
    public let deviceModel: String

    /// iOS version string
    public let iosVersion: String

    /// GPS location at recording start (optional)
    public let location: CaptureLocation?

    /// Attestation level from DCAppAttest ("secure_enclave" or "unverified")
    public let attestationLevel: String

    /// Base64-encoded final hash from hash chain
    public let hashChainFinal: String

    /// Base64-encoded DCAppAttest assertion
    public let assertion: String

    // CodingKeys for snake_case JSON serialization
    private enum CodingKeys: String, CodingKey {
        case type
        case startedAt = "started_at"
        case endedAt = "ended_at"
        case durationMs = "duration_ms"
        case frameCount = "frame_count"
        case depthKeyframeCount = "depth_keyframe_count"
        case resolution
        case codec
        case deviceModel = "device_model"
        case iosVersion = "ios_version"
        case location
        case attestationLevel = "attestation_level"
        case hashChainFinal = "hash_chain_final"
        case assertion
    }
}
]]></code-snippet>
    </interface>

    <interface name="DepthKeyframeData">
      <description>Depth data from Story 7-2 - already compressed with ZLIB</description>
      <source-file>ios/Rial/Core/Capture/DepthKeyframeBuffer.swift</source-file>
      <code-snippet><![CDATA[
public struct DepthKeyframeData: Codable, Sendable {
    /// Array of keyframe metadata (10fps, up to 150 frames for 15s video)
    public let frames: [DepthKeyframe]

    /// Depth map resolution (typically 256x192 for LiDAR)
    public let resolution: CGSize

    /// Gzip-compressed Float32 depth data for all frames
    public let compressedBlob: Data

    /// Uncompressed size in bytes (for decompression buffer allocation)
    public let uncompressedSize: Int

    /// Number of keyframes captured
    public var keyframeCount: Int {
        frames.count
    }

    /// Compression ratio achieved (uncompressed / compressed)
    public var compressionRatio: Double {
        guard compressedBlob.count > 0 else { return 0 }
        return Double(uncompressedSize) / Double(compressedBlob.count)
    }
}

public struct DepthKeyframe: Codable, Sendable, Equatable {
    /// 0-based frame index (0, 1, 2, ... up to 149)
    public let index: Int

    /// Video timestamp from ARFrame (seconds since recording start)
    public let timestamp: TimeInterval

    /// Byte offset in the uncompressed blob where this keyframe's data begins
    public let offset: Int
}
]]></code-snippet>
    </interface>

    <interface name="CaptureStatus">
      <description>Existing status enum from CaptureData - reuse for video</description>
      <source-file>ios/Rial/Models/CaptureData.swift</source-file>
      <code-snippet><![CDATA[
/// Status of a capture in the processing/upload queue.
public enum CaptureStatus: String, Codable, Sendable, CaseIterable {
    /// Capture is being processed (JPEG conversion, compression)
    case processing

    /// Capture is processed and ready for upload
    case pending

    /// Capture is currently being uploaded
    case uploading

    /// Upload is paused (network unavailable, app backgrounded)
    case paused

    /// Upload completed successfully
    case uploaded

    /// Upload failed (will retry)
    case failed

    /// Whether this status indicates the capture is complete
    public var isComplete: Bool {
        self == .uploaded
    }

    /// Whether this status indicates work is in progress
    public var isInProgress: Bool {
        self == .processing || self == .uploading
    }
}
]]></code-snippet>
    </interface>
  </technical-interfaces>

  <existing-code-patterns>
    <pattern name="CaptureStore CoreData Persistence">
      <description>Follow this pattern for video capture CoreData persistence</description>
      <source-file>ios/Rial/Core/Storage/CaptureStore.swift</source-file>
      <code-snippet><![CDATA[
/// CoreData-based persistence for captures with upload queue management.
///
/// Provides persistent storage for captures that survive app restarts, with
/// automatic status tracking and upload queue management.
final class CaptureStore {
    private static let logger = Logger(subsystem: "app.rial", category: "capture-store")

    /// CoreData persistent container
    private let container: NSPersistentContainer

    /// Optional encryption service for at-rest encryption
    private let encryption: CaptureEncryption?

    // MARK: - Public API

    /// Save a new capture to the store.
    func saveCapture(_ capture: CaptureData, status: CaptureStatus = .pending) async throws {
        let context = container.newBackgroundContext()

        try await context.perform {
            let entity = CaptureEntity(context: context)
            entity.id = capture.id
            entity.jpeg = jpegData
            entity.depth = depthData
            entity.metadata = metadataData
            entity.status = status.rawValue
            entity.createdAt = capture.timestamp

            try context.save()
        }

        Self.logger.info("Saved capture: \(capture.id.uuidString)")
    }

    /// Update capture status.
    func updateStatus(_ status: CaptureStatus, for captureId: UUID) async throws {
        let context = container.newBackgroundContext()

        try await context.perform {
            let request = CaptureEntity.fetchRequest()
            request.predicate = NSPredicate(format: "id == %@", captureId as CVarArg)
            request.fetchLimit = 1

            guard let entity = try context.fetch(request).first else {
                throw CaptureStoreError.notFound
            }

            entity.status = status.rawValue
            try context.save()
        }
    }
}
]]></code-snippet>
    </pattern>

    <pattern name="Gzip Compression with Compression Framework">
      <description>Existing compression pattern from DepthKeyframeBuffer - reusable</description>
      <source-file>ios/Rial/Core/Capture/DepthKeyframeBuffer.swift</source-file>
      <code-snippet><![CDATA[
import Compression

/// Compress data using gzip (COMPRESSION_ZLIB).
private func compressBlob(_ data: Data) throws -> Data {
    guard !data.isEmpty else {
        return Data()
    }

    // Allocate destination buffer (same size as source - compression will shrink it)
    let destinationBufferSize = data.count
    let destinationBuffer = UnsafeMutablePointer<UInt8>.allocate(capacity: destinationBufferSize)
    defer { destinationBuffer.deallocate() }

    let compressedSize = data.withUnsafeBytes { sourceBuffer -> Int in
        guard let sourcePointer = sourceBuffer.baseAddress?.assumingMemoryBound(to: UInt8.self) else {
            return 0
        }

        return compression_encode_buffer(
            destinationBuffer,
            destinationBufferSize,
            sourcePointer,
            data.count,
            nil,
            COMPRESSION_ZLIB
        )
    }

    guard compressedSize > 0 else {
        throw DepthKeyframeError.compressionFailed
    }

    return Data(bytes: destinationBuffer, count: compressedSize)
}
]]></code-snippet>
    </pattern>

    <pattern name="JSON Serialization with Snake Case">
      <description>VideoMetadata pattern for snake_case JSON - use for hash chain</description>
      <source-file>ios/Rial/Models/VideoMetadata.swift</source-file>
      <code-snippet><![CDATA[
// Use JSONEncoder with snake_case key strategy for backend API compatibility
let encoder = JSONEncoder()
encoder.keyEncodingStrategy = .convertToSnakeCase
encoder.dateEncodingStrategy = .iso8601

// For Data fields (like hashes), encode as base64
let hashChainJSON = """
{
  "frame_hashes": [\(frameHashes.map { $0.base64EncodedString() }.joined(separator: ","))],
  "checkpoints": [...],
  "final_hash": "\(finalHash.base64EncodedString())"
}
"""
]]></code-snippet>
    </pattern>

    <pattern name="Thumbnail Generation with AVAssetImageGenerator">
      <description>Standard pattern for video thumbnail extraction</description>
      <code-snippet><![CDATA[
import AVFoundation
import UIKit

/// Generate thumbnail from video first frame
func generateThumbnail(from videoURL: URL) async throws -> Data {
    let asset = AVURLAsset(url: videoURL)
    let generator = AVAssetImageGenerator(asset: asset)
    generator.appliesPreferredTrackTransform = true
    generator.maximumSize = CGSize(width: 640, height: 360)

    let time = CMTime(seconds: 0, preferredTimescale: 600)
    let cgImage = try await generator.image(at: time).image

    let uiImage = UIImage(cgImage: cgImage)

    guard let jpegData = uiImage.jpegData(compressionQuality: 0.8) else {
        throw VideoProcessingError.thumbnailGenerationFailed
    }

    return jpegData
}
]]></code-snippet>
    </pattern>

    <pattern name="Background Processing with DispatchQueue">
      <description>Follow this pattern for processing heavy operations</description>
      <source-file>ios/Rial/Core/Capture/VideoRecordingSession.swift</source-file>
      <code-snippet><![CDATA[
/// Serial queue for thread-safe recording operations
private let recordingQueue = DispatchQueue(label: "app.rial.videorecording", qos: .userInitiated)

// Use .userInitiated for processing that user is waiting for
private let compressionQueue = DispatchQueue(label: "app.rial.depth-compression", qos: .userInitiated)

/// Async wrapper for queue-based operations
func compressDepthData(_ data: DepthKeyframeData) async throws -> Data {
    return try await withCheckedThrowingContinuation { continuation in
        compressionQueue.async {
            do {
                let compressedData = try self.compress(data.compressedBlob, algorithm: .zlib)
                continuation.resume(returning: compressedData)
            } catch {
                continuation.resume(throwing: error)
            }
        }
    }
}
]]></code-snippet>
    </pattern>

    <pattern name="Logger Integration">
      <description>Follow this logging pattern throughout</description>
      <code-snippet><![CDATA[
import os.log

final class VideoProcessingPipeline {
    private static let logger = Logger(subsystem: "app.rial", category: "videoprocessing")

    // Log key metrics
    logger.info("Processing video: \(frameCount) frames, \(depthKeyframeCount) depth keyframes")
    logger.debug("Compression ratio: \(String(format: "%.1f", ratio))x")
    logger.warning("Processing exceeded 5s target: \(String(format: "%.1f", duration))s")
    logger.error("Thumbnail generation failed: \(error.localizedDescription)")
}
]]></code-snippet>
    </pattern>
  </existing-code-patterns>

  <files-to-create>
    <file path="ios/Rial/Models/ProcessedVideoCapture.swift">
      <description>New model for processed video capture ready for upload</description>
      <template><![CDATA[
//
//  ProcessedVideoCapture.swift
//  Rial
//
//  Created by RealityCam on 2025-11-27.
//
//  Processed video capture ready for upload to backend.
//

import Foundation

// MARK: - ProcessedVideoCapture

/// Processed video capture ready for upload.
///
/// Contains all components needed for backend video verification:
/// - Video file (already encoded by AVAssetWriter)
/// - Compressed depth data (gzip)
/// - Serialized hash chain (JSON)
/// - Metadata with attestation (JSON)
/// - Thumbnail (JPEG)
///
/// ## Usage
/// ```swift
/// let pipeline = VideoProcessingPipeline()
/// let processed = try await pipeline.process(result: videoResult) { progress in
///     updateUI(progress: progress)
/// }
/// // processed.status == .pendingUpload
/// ```
public struct ProcessedVideoCapture: Identifiable, Sendable {
    /// Unique identifier for this capture
    public let id: UUID

    /// URL to local video file (H.264/HEVC encoded)
    public let videoURL: URL

    /// Gzip-compressed depth keyframe data
    public let compressedDepthData: Data

    /// Serialized hash chain as JSON
    public let hashChainJSON: Data

    /// Serialized metadata with attestation as JSON
    public let metadataJSON: Data

    /// JPEG thumbnail (640x360, 80% quality)
    public let thumbnailData: Data

    /// Capture creation timestamp
    public let createdAt: Date

    /// Current upload status
    public var status: VideoCaptureStatus

    /// Frame count from recording
    public let frameCount: Int

    /// Depth keyframe count
    public let depthKeyframeCount: Int

    /// Duration in milliseconds
    public let durationMs: Int64

    /// Whether this is a partial (interrupted) recording
    public let isPartial: Bool

    /// Creates a new ProcessedVideoCapture.
    public init(
        id: UUID = UUID(),
        videoURL: URL,
        compressedDepthData: Data,
        hashChainJSON: Data,
        metadataJSON: Data,
        thumbnailData: Data,
        createdAt: Date = Date(),
        status: VideoCaptureStatus = .pendingUpload,
        frameCount: Int,
        depthKeyframeCount: Int,
        durationMs: Int64,
        isPartial: Bool
    ) {
        self.id = id
        self.videoURL = videoURL
        self.compressedDepthData = compressedDepthData
        self.hashChainJSON = hashChainJSON
        self.metadataJSON = metadataJSON
        self.thumbnailData = thumbnailData
        self.createdAt = createdAt
        self.status = status
        self.frameCount = frameCount
        self.depthKeyframeCount = depthKeyframeCount
        self.durationMs = durationMs
        self.isPartial = isPartial
    }

    /// Total size of all data in bytes
    public var totalSizeBytes: Int {
        // Video file size + depth + hash chain + metadata + thumbnail
        let videoSize = (try? FileManager.default.attributesOfItem(atPath: videoURL.path)[.size] as? Int) ?? 0
        return videoSize + compressedDepthData.count + hashChainJSON.count + metadataJSON.count + thumbnailData.count
    }

    /// Human-readable size string
    public var totalSizeFormatted: String {
        ByteCountFormatter.string(fromByteCount: Int64(totalSizeBytes), countStyle: .file)
    }
}

// MARK: - VideoCaptureStatus

/// Status of a video capture in the processing/upload queue.
public enum VideoCaptureStatus: String, Codable, Sendable {
    /// Video is being processed
    case processing

    /// Processing complete, ready for upload
    case pendingUpload = "pending_upload"

    /// Currently uploading
    case uploading

    /// Upload complete
    case uploaded

    /// Processing or upload failed
    case failed
}
]]></template>
    </file>

    <file path="ios/Rial/Core/Capture/VideoProcessingPipeline.swift">
      <description>Main processing pipeline service</description>
      <template><![CDATA[
//
//  VideoProcessingPipeline.swift
//  Rial
//
//  Created by RealityCam on 2025-11-27.
//
//  Pipeline for processing video recordings into upload-ready packages.
//

import Foundation
import AVFoundation
import UIKit
import Compression
import os.log

// MARK: - VideoProcessingError

/// Errors that can occur during video processing.
public enum VideoProcessingError: Error, LocalizedError {
    /// Depth data compression failed
    case compressionFailed

    /// Thumbnail generation failed
    case thumbnailGenerationFailed

    /// JSON serialization failed
    case serializationFailed(Error)

    /// Invalid input (missing required data)
    case invalidInput(String)

    public var errorDescription: String? {
        switch self {
        case .compressionFailed:
            return "Failed to compress depth data"
        case .thumbnailGenerationFailed:
            return "Failed to generate video thumbnail"
        case .serializationFailed(let error):
            return "Failed to serialize data: \(error.localizedDescription)"
        case .invalidInput(let reason):
            return "Invalid input: \(reason)"
        }
    }
}

// MARK: - VideoProcessingPipeline

/// Pipeline for processing video recordings into upload-ready packages.
///
/// Takes VideoRecordingResult and produces ProcessedVideoCapture containing:
/// - Video file (already encoded)
/// - Compressed depth data (gzip)
/// - Serialized hash chain (JSON)
/// - Metadata with attestation (JSON)
/// - Thumbnail (JPEG)
///
/// ## Performance Targets
/// - Total processing time: < 5 seconds for 15s video
/// - Memory: No additional spike above 50MB
/// - UI remains responsive (processing on background queue)
///
/// ## Usage
/// ```swift
/// let pipeline = VideoProcessingPipeline()
/// let processed = try await pipeline.process(result: recordingResult) { progress in
///     print("Progress: \(Int(progress * 100))%")
/// }
/// ```
public final class VideoProcessingPipeline {

    // MARK: - Properties

    /// Logger for processing events
    private static let logger = Logger(subsystem: "app.rial", category: "videoprocessing")

    /// Queue for CPU-intensive operations
    private let processingQueue = DispatchQueue(label: "app.rial.videoprocessing", qos: .userInitiated)

    // MARK: - Progress Stages

    /// Progress stages and their weights
    private enum ProcessingStage: CaseIterable {
        case depthCompression   // 40%
        case thumbnail          // 20%
        case hashChainSerialize // 20%
        case metadataSerialize  // 10%
        case assembly           // 10%

        var weight: Double {
            switch self {
            case .depthCompression: return 0.40
            case .thumbnail: return 0.20
            case .hashChainSerialize: return 0.20
            case .metadataSerialize: return 0.10
            case .assembly: return 0.10
            }
        }

        var startProgress: Double {
            let allCases = ProcessingStage.allCases
            let index = allCases.firstIndex(of: self) ?? 0
            return allCases[..<index].reduce(0) { $0 + $1.weight }
        }
    }

    // MARK: - Initialization

    public init() {
        Self.logger.debug("VideoProcessingPipeline initialized")
    }

    // MARK: - Public Methods

    /// Process a video recording result into an upload-ready package.
    ///
    /// - Parameters:
    ///   - result: VideoRecordingResult from recording session
    ///   - onProgress: Progress callback (0.0 - 1.0)
    /// - Returns: ProcessedVideoCapture ready for upload
    /// - Throws: `VideoProcessingError` if processing fails
    public func process(
        result: VideoRecordingResult,
        onProgress: ((Double) -> Void)? = nil
    ) async throws -> ProcessedVideoCapture {
        let startTime = CFAbsoluteTimeGetCurrent()

        Self.logger.info("Processing video: \(result.frameCount) frames, \(result.depthKeyframeCount) depth keyframes, \(String(format: "%.1f", result.duration))s")

        // Report initial progress
        onProgress?(0.0)

        // Stage 1: Compress depth data (40%)
        let compressedDepth: Data
        if let depthData = result.depthKeyframeData {
            Self.logger.debug("Starting depth compression: \(depthData.compressedBlob.count) bytes input")
            compressedDepth = depthData.compressedBlob  // Already compressed by DepthKeyframeBuffer
            Self.logger.debug("Depth data size: \(compressedDepth.count) bytes")
        } else {
            Self.logger.warning("No depth keyframe data available")
            compressedDepth = Data()
        }
        onProgress?(ProcessingStage.depthCompression.startProgress + ProcessingStage.depthCompression.weight)

        // Stage 2: Generate thumbnail (20%)
        let thumbnailData: Data
        do {
            thumbnailData = try await generateThumbnail(from: result.videoURL)
            Self.logger.debug("Thumbnail generated: \(thumbnailData.count) bytes")
        } catch {
            Self.logger.warning("Thumbnail generation failed, using empty data: \(error.localizedDescription)")
            thumbnailData = Data()  // Continue without thumbnail
        }
        onProgress?(ProcessingStage.thumbnail.startProgress + ProcessingStage.thumbnail.weight)

        // Stage 3: Serialize hash chain (20%)
        let hashChainJSON: Data
        if let hashChain = result.hashChainData {
            hashChainJSON = try serializeHashChain(hashChain)
            Self.logger.debug("Hash chain serialized: \(hashChainJSON.count) bytes")
        } else {
            Self.logger.warning("No hash chain data available")
            hashChainJSON = Data()
        }
        onProgress?(ProcessingStage.hashChainSerialize.startProgress + ProcessingStage.hashChainSerialize.weight)

        // Stage 4: Serialize metadata (10%)
        let metadataJSON = try serializeMetadata(result.metadata)
        Self.logger.debug("Metadata serialized: \(metadataJSON.count) bytes")
        onProgress?(ProcessingStage.metadataSerialize.startProgress + ProcessingStage.metadataSerialize.weight)

        // Stage 5: Assembly (10%)
        let processed = ProcessedVideoCapture(
            videoURL: result.videoURL,
            compressedDepthData: compressedDepth,
            hashChainJSON: hashChainJSON,
            metadataJSON: metadataJSON,
            thumbnailData: thumbnailData,
            createdAt: result.startedAt,
            status: .pendingUpload,
            frameCount: result.frameCount,
            depthKeyframeCount: result.depthKeyframeCount,
            durationMs: Int64(result.duration * 1000),
            isPartial: result.isPartial
        )
        onProgress?(1.0)

        let totalTime = CFAbsoluteTimeGetCurrent() - startTime
        Self.logger.info("Video processing complete: \(String(format: "%.2f", totalTime))s, total size: \(processed.totalSizeFormatted)")

        // Warn if exceeded target
        if totalTime > 5.0 {
            Self.logger.warning("Processing exceeded 5s target: \(String(format: "%.2f", totalTime))s")
        }

        return processed
    }

    // MARK: - Component Methods

    /// Generate thumbnail from video first frame.
    ///
    /// - Parameter videoURL: URL to video file
    /// - Returns: JPEG data (640x360, 80% quality)
    /// - Throws: `VideoProcessingError.thumbnailGenerationFailed`
    public func generateThumbnail(from videoURL: URL) async throws -> Data {
        let asset = AVURLAsset(url: videoURL)
        let generator = AVAssetImageGenerator(asset: asset)
        generator.appliesPreferredTrackTransform = true
        generator.maximumSize = CGSize(width: 640, height: 360)

        let time = CMTime(seconds: 0, preferredTimescale: 600)

        do {
            let cgImage = try await generator.image(at: time).image
            let uiImage = UIImage(cgImage: cgImage)

            guard let jpegData = uiImage.jpegData(compressionQuality: 0.8) else {
                throw VideoProcessingError.thumbnailGenerationFailed
            }

            Self.logger.info("Generated thumbnail: \(jpegData.count) bytes")
            return jpegData

        } catch let error as VideoProcessingError {
            throw error
        } catch {
            Self.logger.error("Thumbnail generation failed: \(error.localizedDescription)")
            throw VideoProcessingError.thumbnailGenerationFailed
        }
    }

    /// Serialize hash chain to JSON.
    ///
    /// Produces JSON with base64-encoded hashes for backend compatibility.
    ///
    /// - Parameter chain: HashChainData from recording
    /// - Returns: JSON data
    /// - Throws: `VideoProcessingError.serializationFailed`
    public func serializeHashChain(_ chain: HashChainData) throws -> Data {
        do {
            // Create serializable structure
            let serializable = SerializableHashChain(
                frameHashes: chain.frameHashes.map { $0.base64EncodedString() },
                checkpoints: chain.checkpoints.map { checkpoint in
                    SerializableCheckpoint(
                        index: checkpoint.index,
                        frameNumber: checkpoint.frameNumber,
                        hash: checkpoint.hash.base64EncodedString(),
                        timestamp: checkpoint.timestamp
                    )
                },
                finalHash: chain.finalHash.base64EncodedString(),
                frameCount: chain.frameCount,
                checkpointCount: chain.checkpointCount
            )

            let encoder = JSONEncoder()
            encoder.keyEncodingStrategy = .convertToSnakeCase
            encoder.outputFormatting = [.sortedKeys]

            return try encoder.encode(serializable)

        } catch {
            Self.logger.error("Hash chain serialization failed: \(error.localizedDescription)")
            throw VideoProcessingError.serializationFailed(error)
        }
    }

    /// Serialize metadata to JSON.
    ///
    /// Uses VideoMetadata's built-in Codable with snake_case keys.
    ///
    /// - Parameter metadata: VideoMetadata from recording
    /// - Returns: JSON data
    /// - Throws: `VideoProcessingError.serializationFailed`
    public func serializeMetadata(_ metadata: VideoMetadata) throws -> Data {
        do {
            let encoder = JSONEncoder()
            // Note: VideoMetadata has custom encode() that handles snake_case
            return try encoder.encode(metadata)

        } catch {
            Self.logger.error("Metadata serialization failed: \(error.localizedDescription)")
            throw VideoProcessingError.serializationFailed(error)
        }
    }
}

// MARK: - Serializable Types

/// Serializable hash chain for JSON encoding
private struct SerializableHashChain: Codable {
    let frameHashes: [String]
    let checkpoints: [SerializableCheckpoint]
    let finalHash: String
    let frameCount: Int
    let checkpointCount: Int
}

/// Serializable checkpoint for JSON encoding
private struct SerializableCheckpoint: Codable {
    let index: Int
    let frameNumber: Int
    let hash: String
    let timestamp: TimeInterval
}
]]></template>
    </file>

    <file path="ios/RialTests/Capture/VideoProcessingPipelineTests.swift">
      <description>Unit tests for VideoProcessingPipeline</description>
      <template><![CDATA[
//
//  VideoProcessingPipelineTests.swift
//  RialTests
//
//  Unit tests for VideoProcessingPipeline.
//

import XCTest
@testable import Rial

final class VideoProcessingPipelineTests: XCTestCase {

    var sut: VideoProcessingPipeline!

    override func setUp() {
        super.setUp()
        sut = VideoProcessingPipeline()
    }

    override func tearDown() {
        sut = nil
        super.tearDown()
    }

    // MARK: - Hash Chain Serialization Tests

    func testSerializeHashChain_ProducesValidJSON() throws {
        // Given
        let hashChainData = createMockHashChainData()

        // When
        let jsonData = try sut.serializeHashChain(hashChainData)

        // Then
        XCTAssertGreaterThan(jsonData.count, 0)

        // Verify it's valid JSON
        let json = try JSONSerialization.jsonObject(with: jsonData) as? [String: Any]
        XCTAssertNotNil(json)
        XCTAssertNotNil(json?["frame_hashes"])
        XCTAssertNotNil(json?["checkpoints"])
        XCTAssertNotNil(json?["final_hash"])
    }

    func testSerializeHashChain_IncludesAllFrameHashes() throws {
        // Given
        let hashChainData = createMockHashChainData(frameCount: 10)

        // When
        let jsonData = try sut.serializeHashChain(hashChainData)
        let json = try JSONSerialization.jsonObject(with: jsonData) as? [String: Any]

        // Then
        let frameHashes = json?["frame_hashes"] as? [String]
        XCTAssertEqual(frameHashes?.count, 10)
    }

    func testSerializeHashChain_IncludesCheckpoints() throws {
        // Given
        let hashChainData = createMockHashChainData(checkpointCount: 2)

        // When
        let jsonData = try sut.serializeHashChain(hashChainData)
        let json = try JSONSerialization.jsonObject(with: jsonData) as? [String: Any]

        // Then
        let checkpoints = json?["checkpoints"] as? [[String: Any]]
        XCTAssertEqual(checkpoints?.count, 2)
    }

    func testSerializeHashChain_UsesBase64ForHashes() throws {
        // Given
        let hashChainData = createMockHashChainData()

        // When
        let jsonData = try sut.serializeHashChain(hashChainData)
        let json = try JSONSerialization.jsonObject(with: jsonData) as? [String: Any]

        // Then
        let finalHash = json?["final_hash"] as? String
        XCTAssertNotNil(finalHash)
        // Base64 strings should be decodable
        XCTAssertNotNil(Data(base64Encoded: finalHash!))
    }

    // MARK: - Metadata Serialization Tests

    func testSerializeMetadata_ProducesSnakeCaseJSON() throws {
        // Given
        let metadata = createMockVideoMetadata()

        // When
        let jsonData = try sut.serializeMetadata(metadata)
        let jsonString = String(data: jsonData, encoding: .utf8)!

        // Then
        XCTAssertTrue(jsonString.contains("started_at"))
        XCTAssertTrue(jsonString.contains("ended_at"))
        XCTAssertTrue(jsonString.contains("duration_ms"))
        XCTAssertTrue(jsonString.contains("frame_count"))
        XCTAssertTrue(jsonString.contains("depth_keyframe_count"))
        XCTAssertTrue(jsonString.contains("device_model"))
        XCTAssertTrue(jsonString.contains("ios_version"))
        XCTAssertTrue(jsonString.contains("attestation_level"))
        XCTAssertTrue(jsonString.contains("hash_chain_final"))
    }

    func testSerializeMetadata_IncludesAttestation() throws {
        // Given
        let metadata = createMockVideoMetadata()

        // When
        let jsonData = try sut.serializeMetadata(metadata)
        let json = try JSONSerialization.jsonObject(with: jsonData) as? [String: Any]

        // Then
        XCTAssertNotNil(json?["assertion"])
        XCTAssertNotNil(json?["hash_chain_final"])
    }

    // MARK: - ProcessedVideoCapture Tests

    func testProcessedVideoCapture_TotalSizeCalculation() {
        // Given
        let processed = ProcessedVideoCapture(
            videoURL: URL(fileURLWithPath: "/tmp/test.mov"),
            compressedDepthData: Data(repeating: 0, count: 1000),
            hashChainJSON: Data(repeating: 0, count: 500),
            metadataJSON: Data(repeating: 0, count: 200),
            thumbnailData: Data(repeating: 0, count: 300),
            frameCount: 450,
            depthKeyframeCount: 150,
            durationMs: 15000,
            isPartial: false
        )

        // Then
        // totalSizeBytes includes video file size (0 for non-existent) + other data
        XCTAssertGreaterThanOrEqual(processed.totalSizeBytes, 2000)
    }

    func testProcessedVideoCapture_StatusDefaults() {
        // Given/When
        let processed = ProcessedVideoCapture(
            videoURL: URL(fileURLWithPath: "/tmp/test.mov"),
            compressedDepthData: Data(),
            hashChainJSON: Data(),
            metadataJSON: Data(),
            thumbnailData: Data(),
            frameCount: 450,
            depthKeyframeCount: 150,
            durationMs: 15000,
            isPartial: false
        )

        // Then
        XCTAssertEqual(processed.status, .pendingUpload)
    }

    // MARK: - Helpers

    private func createMockHashChainData(frameCount: Int = 5, checkpointCount: Int = 1) -> HashChainData {
        var frameHashes: [Data] = []
        for _ in 0..<frameCount {
            frameHashes.append(Data(repeating: UInt8.random(in: 0...255), count: 32))
        }

        var checkpoints: [HashCheckpoint] = []
        for i in 0..<checkpointCount {
            checkpoints.append(HashCheckpoint(
                index: i,
                frameNumber: (i + 1) * 150,
                hash: Data(repeating: UInt8.random(in: 0...255), count: 32),
                timestamp: TimeInterval((i + 1) * 5)
            ))
        }

        return HashChainData(
            frameHashes: frameHashes,
            checkpoints: checkpoints,
            finalHash: frameHashes.last ?? Data()
        )
    }

    private func createMockVideoMetadata() -> VideoMetadata {
        return VideoMetadata(
            type: "video",
            startedAt: Date(),
            endedAt: Date().addingTimeInterval(15),
            durationMs: 15000,
            frameCount: 450,
            depthKeyframeCount: 150,
            resolution: Resolution(width: 1920, height: 1080),
            codec: "hevc",
            deviceModel: "iPhone 15 Pro",
            iosVersion: "17.4",
            location: nil,
            attestationLevel: "secure_enclave",
            hashChainFinal: "base64hashstring",
            assertion: "base64assertionstring"
        )
    }
}
]]></template>
    </file>
  </files-to-create>

  <files-to-modify>
    <file path="ios/Rial.xcodeproj/project.pbxproj">
      <description>Add new files to Xcode project</description>
      <changes>
        - Add ProcessedVideoCapture.swift to Models group
        - Add VideoProcessingPipeline.swift to Core/Capture group
        - Add VideoProcessingPipelineTests.swift to RialTests/Capture group
      </changes>
    </file>
    <file path="ios/Rial/Core/Storage/CaptureStore.swift" optional="true">
      <description>Optionally extend for video capture storage (may be deferred to Story 7-8)</description>
      <changes>
        - Add VideoCaptureEntity if needed for separate video storage
        - Or reuse existing CaptureEntity with type discriminator
      </changes>
    </file>
  </files-to-modify>

  <development-constraints>
    <constraint category="performance">
      <title>Processing Time Budget</title>
      <description>Total processing must complete in under 5 seconds for 15s video</description>
      <budget>
        - Depth compression: ~0s (already compressed by DepthKeyframeBuffer)
        - Thumbnail generation: ~0.5s
        - Hash chain serialization: ~0.2s
        - Metadata serialization: ~0.1s
        - Assembly: ~0.1s
        - Buffer for variability: ~4.1s
      </budget>
    </constraint>
    <constraint category="memory">
      <title>Memory Usage</title>
      <description>No additional memory spike above 50MB during processing</description>
      <notes>
        - Depth data already compressed (~10MB)
        - Video file streams from disk (not loaded in memory)
        - Thumbnail is small (~50KB)
        - JSON serialization uses streaming where possible
      </notes>
    </constraint>
    <constraint category="thread-safety">
      <title>Background Processing</title>
      <description>All heavy operations on background queue</description>
      <notes>
        - Use DispatchQueue with .userInitiated QoS
        - Progress callbacks dispatch to main thread
        - UI remains responsive during processing
      </notes>
    </constraint>
    <constraint category="fail-safe">
      <title>Graceful Degradation</title>
      <description>Pipeline should continue even if optional components fail</description>
      <notes>
        - Missing depth data: Continue with empty Data
        - Thumbnail failure: Continue with empty thumbnail
        - Never fail entire capture for optional component failure
      </notes>
    </constraint>
  </development-constraints>

  <dependencies>
    <internal-dependency>
      <name>CaptureStore (Story 6-9)</name>
      <description>CoreData-based persistence for captures with upload queue management</description>
      <note>Provides CaptureStore patterns for video capture CoreData persistence - save/update operations, status tracking, offline queue integration</note>
    </internal-dependency>
    <internal-dependency>
      <name>DepthKeyframeBuffer</name>
      <description>Provides already-compressed depth data via finalize()</description>
      <note>Depth compression happens during finalize() in Story 7-2, not in this pipeline</note>
    </internal-dependency>
    <internal-dependency>
      <name>HashChainService</name>
      <description>Provides HashChainData with all frame hashes and checkpoints</description>
      <note>HashChainData is already Codable but needs custom serialization for base64 hashes</note>
    </internal-dependency>
    <internal-dependency>
      <name>VideoMetadataCollector</name>
      <description>Provides VideoMetadata with snake_case JSON serialization</description>
      <note>VideoMetadata has custom encode() for ISO8601 dates and snake_case keys</note>
    </internal-dependency>
    <internal-dependency>
      <name>VideoAttestationService</name>
      <description>Provides VideoAttestation in VideoRecordingResult</description>
      <note>Attestation data is already base64-encoded in VideoMetadata</note>
    </internal-dependency>
    <external-dependency>
      <name>AVFoundation</name>
      <description>AVAssetImageGenerator for thumbnail extraction</description>
    </external-dependency>
    <external-dependency>
      <name>UIKit</name>
      <description>UIImage for JPEG compression</description>
    </external-dependency>
    <external-dependency>
      <name>Compression</name>
      <description>COMPRESSION_ZLIB for gzip (if additional compression needed)</description>
    </external-dependency>
  </dependencies>

  <testing-context>
    <testing-framework>XCTest</testing-framework>
    <test-patterns>
      <pattern>Use XCTSkip for device-only tests</pattern>
      <pattern>Create fixture data for hash chains and metadata</pattern>
      <pattern>Test JSON output matches expected snake_case format</pattern>
      <pattern>Verify base64 encoding for hash values</pattern>
    </test-patterns>
    <coverage-target>80% for VideoProcessingPipeline</coverage-target>
    <test-files>
      <file>ios/RialTests/Capture/VideoProcessingPipelineTests.swift</file>
    </test-files>
  </testing-context>

  <implementation-notes>
    <note priority="high">
      DepthKeyframeBuffer already compresses depth data in finalize() - do NOT re-compress
    </note>
    <note priority="high">
      VideoMetadata already has snake_case JSON serialization - just encode directly
    </note>
    <note priority="high">
      HashChainData needs custom serialization to convert Data hashes to base64 strings
    </note>
    <note priority="medium">
      Thumbnail generation uses AVAssetImageGenerator.image(at:) - async in iOS 16+
    </note>
    <note priority="medium">
      Progress callback should dispatch to main thread for UI updates
    </note>
    <note priority="low">
      CoreData persistence may be deferred to Story 7-8 (Video Upload) if needed
    </note>
  </implementation-notes>

  <validation-checklist>
    <item>VideoProcessingPipeline.process() returns valid ProcessedVideoCapture</item>
    <item>Processing completes in under 5 seconds for 15s video</item>
    <item>Progress callback provides updates at each stage</item>
    <item>Hash chain JSON includes all frame hashes as base64</item>
    <item>Hash chain JSON includes checkpoints with correct structure</item>
    <item>Metadata JSON uses snake_case keys</item>
    <item>Metadata JSON includes ISO8601 timestamps</item>
    <item>Thumbnail is JPEG, 640x360, ~50KB</item>
    <item>Pipeline continues if thumbnail fails (graceful degradation)</item>
    <item>Unit tests achieve 80% coverage</item>
  </validation-checklist>
</story-context>
